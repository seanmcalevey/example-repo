{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_mlp_function.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPoh1DMuUBr+zljv2aBAxBU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seanmcalevey/example-repo/blob/master/keras_mlp_function.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0ktKwDlJ-rz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def keras_mlp(train_matrix, val_matrix, y_train, y_val, hidden_layers=3,\n",
        "              neuron_layers=(128, 128, 128), dropout_layers=(None, None, None),\n",
        "              epochs=20, batch_size=32, patience=2, activation='relu', optimizer='adam',\n",
        "              batch_norm=True, restore_best_weights=True):\n",
        "\n",
        "  # Manage warnings:\n",
        "\n",
        "  import warnings\n",
        "\n",
        "  warnings.filterwarnings('ignore')\n",
        "  \n",
        "  # Import relevant modules\n",
        "\n",
        "  import numpy as np\n",
        "  import pandas as pd\n",
        "  from keras.models import Sequential\n",
        "  from keras.layers import Dense, Dropout, BatchNormalization\n",
        "  from sklearn.metrics import classification_report, f1_score\n",
        "  from keras.callbacks import EarlyStopping\n",
        "\n",
        "  if hidden_layers == 4:\n",
        "    \n",
        "    neurons_1, neurons_2, neurons_3, neurons_4 = neuron_layers[0], neuron_layers[1], neuron_layers[2], neuron_layers[3]\n",
        "\n",
        "    dropout_1, dropout_2, dropout_3, dropout_4 = dropout_layers[0], dropout_layers[1], dropout_layers[2], dropout_layers[3]\n",
        "\n",
        "  elif hidden_layers == 3:\n",
        "\n",
        "    # Unpack hyperparameters:\n",
        "\n",
        "    neurons_1, neurons_2, neurons_3 = neuron_layers[0], neuron_layers[1], neuron_layers[2]\n",
        "\n",
        "    dropout_1, dropout_2, dropout_3 = dropout_layers[0], dropout_layers[1], dropout_layers[2]\n",
        "  \n",
        "  elif hidden_layers == 2:\n",
        "\n",
        "    neurons_1, neurons_2 = neuron_layers[0], neuron_layers[1]\n",
        "\n",
        "    dropout_1, dropout_2 = dropout_layers[0], dropout_layers[1]\n",
        "  \n",
        "  elif hidden_layers == 1:\n",
        "\n",
        "    neurons_1 = neuron_layers[0]\n",
        "\n",
        "    dropout_1 = dropout_layers[0]\n",
        "\n",
        "  # Set input dimensions\n",
        "\n",
        "  input_dim = train_matrix.shape[1]\n",
        "\n",
        "  # Build model\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  # Layer 1\n",
        "  model.add(Dense(neurons_1, input_dim=input_dim, activation=activation))\n",
        "  if dropout_1 != None:\n",
        "    model.add(Dropout(dropout_1))\n",
        "  if batch_norm == True:\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "  # Layer 2 (Optional)\n",
        "  if hidden_layers >= 2:\n",
        "\n",
        "    model.add(Dense(neurons_2, activation=activation))\n",
        "    if dropout_2 != None:\n",
        "      model.add(Dropout(dropout_2))\n",
        "    if batch_norm == True:\n",
        "      model.add(BatchNormalization())\n",
        "\n",
        "  # Layer 3 (Optional)\n",
        "  if hidden_layers >= 3:\n",
        "  \n",
        "    model.add(Dense(neurons_3, activation=activation))\n",
        "    if dropout_3 != None:\n",
        "      model.add(Dropout(dropout_3))\n",
        "    if batch_norm == True:\n",
        "      model.add(BatchNormalization())\n",
        "  \n",
        "  # Layer 4 (Optional)\n",
        "  if hidden_layers >= 4:\n",
        "\n",
        "    model.add(Dense(neurons_4, activation=activation))\n",
        "    if dropout_4 != None:\n",
        "      model.add(Dropout(dropout_4))\n",
        "    if batch_norm == True:\n",
        "      model.add(BatchNormalization())\n",
        "\n",
        "  # Output Layer\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "  # Fit and Score Model\n",
        "\n",
        "  callback = EarlyStopping(monitor='val_loss', min_delta=0, patience=patience, restore_best_weights=restore_best_weights)\n",
        "\n",
        "  model_instance = model.fit(train_matrix, y_train, batch_size=batch_size, epochs=epochs, callbacks=[callback], validation_split=0.2)\n",
        "\n",
        "  # Small function to binarize predicted outputs from model\n",
        "\n",
        "  def binizer(x):\n",
        "    if x > 0.5:\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "\n",
        "  # Get train score\n",
        "\n",
        "  y_train_pred = model.predict(train_matrix)\n",
        "\n",
        "  y_train_pred_bin = pd.Series(list(y_train_pred.reshape(-1,1))).apply(lambda x: binizer(x))\n",
        "\n",
        "  train_f1 = f1_score(y_train, y_train_pred_bin)\n",
        "\n",
        "  train_score = model.evaluate(train_matrix, y_train)[1]\n",
        "\n",
        "  # Get validation score\n",
        "\n",
        "  y_val_pred = model.predict(val_matrix)\n",
        "\n",
        "  y_val_pred_bin = pd.Series(list(y_val_pred.reshape(-1,1))).apply(lambda x: binizer(x))\n",
        "\n",
        "  val_f1 = f1_score(y_val, y_val_pred_bin)\n",
        "\n",
        "  val_score = model.evaluate(val_matrix, y_val)[1]\n",
        "\n",
        "  return model_instance, train_score, train_f1, val_score, val_f1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VotvnJIQEWjE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def keras_mlp_generator(generator, val_generator, train_matrix, val_matrix, hidden_layers=3, neuron_layers=(128, 128, 128),\n",
        "                        dropout_layers=(None, None, None), epochs=20, batch_size=32, patience=2, activation='relu', batch_norm=True,\n",
        "                        restore_best_weights=True, learning_rate=0.01):\n",
        "  \n",
        "  import math\n",
        "\n",
        "  # Manage warnings:\n",
        "\n",
        "  import warnings\n",
        "\n",
        "  warnings.filterwarnings('ignore')\n",
        "  \n",
        "  # Import relevant modules\n",
        "\n",
        "  import numpy as np\n",
        "  import pandas as pd\n",
        "  from keras.models import Sequential\n",
        "  from keras.layers import Dense, Dropout, BatchNormalization\n",
        "  from sklearn.metrics import classification_report, f1_score\n",
        "  from keras.callbacks import EarlyStopping\n",
        "  from keras.optimizers import Adam\n",
        "\n",
        "  if hidden_layers == 4:\n",
        "    \n",
        "    neurons_1, neurons_2, neurons_3, neurons_4 = neuron_layers[0], neuron_layers[1], neuron_layers[2], neuron_layers[3]\n",
        "\n",
        "    dropout_1, dropout_2, dropout_3, dropout_4 = dropout_layers[0], dropout_layers[1], dropout_layers[2], dropout_layers[3]\n",
        "\n",
        "  elif hidden_layers == 3:\n",
        "\n",
        "    # Unpack hyperparameters:\n",
        "\n",
        "    neurons_1, neurons_2, neurons_3 = neuron_layers[0], neuron_layers[1], neuron_layers[2]\n",
        "\n",
        "    dropout_1, dropout_2, dropout_3 = dropout_layers[0], dropout_layers[1], dropout_layers[2]\n",
        "  \n",
        "  elif hidden_layers == 2:\n",
        "\n",
        "    neurons_1, neurons_2 = neuron_layers[0], neuron_layers[1]\n",
        "\n",
        "    dropout_1, dropout_2 = dropout_layers[0], dropout_layers[1]\n",
        "  \n",
        "  elif hidden_layers == 1:\n",
        "\n",
        "    neurons_1 = neuron_layers[0]\n",
        "\n",
        "    dropout_1 = dropout_layers[0]\n",
        "\n",
        "  # Set input dimensions\n",
        "\n",
        "  input_dim = train_matrix.shape[1]\n",
        "\n",
        "  # Build model\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  # Layer 1\n",
        "  model.add(Dense(neurons_1, input_dim=input_dim, activation=activation))\n",
        "  if dropout_1 != None:\n",
        "    model.add(Dropout(dropout_1))\n",
        "  if batch_norm == True:\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "  # Layer 2 (Optional)\n",
        "  if hidden_layers >= 2:\n",
        "\n",
        "    model.add(Dense(neurons_2, activation=activation))\n",
        "    if dropout_2 != None:\n",
        "      model.add(Dropout(dropout_2))\n",
        "    if batch_norm == True:\n",
        "      model.add(BatchNormalization())\n",
        "\n",
        "  # Layer 3 (Optional)\n",
        "  if hidden_layers >= 3:\n",
        "  \n",
        "    model.add(Dense(neurons_3, activation=activation))\n",
        "    if dropout_3 != None:\n",
        "      model.add(Dropout(dropout_3))\n",
        "    if batch_norm == True:\n",
        "      model.add(BatchNormalization())\n",
        "  \n",
        "  # Layer 4 (Optional)\n",
        "  if hidden_layers >= 4:\n",
        "\n",
        "    model.add(Dense(neurons_4, activation=activation))\n",
        "    if dropout_4 != None:\n",
        "      model.add(Dropout(dropout_4))\n",
        "    if batch_norm == True:\n",
        "      model.add(BatchNormalization())\n",
        "\n",
        "  # Output Layer\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate), metrics=['accuracy'])\n",
        "\n",
        "  # Fit and Score Model\n",
        "\n",
        "  callback = EarlyStopping(monitor='val_loss', min_delta=0, patience=patience, restore_best_weights=restore_best_weights)\n",
        "\n",
        "  model_instance = model.fit_generator(generator, steps_per_epoch=math.ceil(train_matrix.shape[0]/batch_size), epochs=epochs, callbacks=[callback],\n",
        "                                       validation_data=val_generator, validation_steps=math.ceil(val_matrix.shape[0]/batch_size))\n",
        "\n",
        "  # Small function to binarize predicted outputs from model\n",
        "\n",
        "  def binizer(x):\n",
        "    if x > 0.5:\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "\n",
        "  # Get train score\n",
        "\n",
        "  y_train_pred = model.predict(train_matrix)\n",
        "\n",
        "  y_train_pred_bin = pd.Series(list(y_train_pred.reshape(-1,1))).apply(lambda x: binizer(x))\n",
        "\n",
        "  train_f1 = f1_score(y_train, y_train_pred_bin)\n",
        "\n",
        "  train_score = model.evaluate(train_matrix, y_train)[1]\n",
        "\n",
        "  # Get validation score\n",
        "\n",
        "  y_val_pred = model.predict(val_matrix)\n",
        "\n",
        "  y_val_pred_bin = pd.Series(list(y_val_pred.reshape(-1,1))).apply(lambda x: binizer(x))\n",
        "\n",
        "  val_f1 = f1_score(y_val, y_val_pred_bin)\n",
        "\n",
        "  val_score = model.evaluate(val_matrix, y_val)[1]\n",
        "\n",
        "  return model_instance, train_score, train_f1, val_score, val_f1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0H-QOD5ozN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}